receivers:
  otlp:
    protocols:
      grpc:
        endpoint: "0.0.0.0:4317"
      http:
        endpoint: "0.0.0.0:4318"

exporters:
  debug: {}

  # Honeycomb Exporters
  otlphttp/honeycomb-express-app:
    endpoint: "https://api.honeycomb.io"
    headers:
      "x-honeycomb-team": "cEFfn4DGWSCFHNfbqecX5a"
      "x-honeycomb-dataset": "express-opentelemetry-app-dataset"

  otlphttp/honeycomb-express-app-2:
    endpoint: "https://api.honeycomb.io"
    headers:
      "x-honeycomb-team": "gDJYf9KL8XdOALiIqnvqVD"
      "x-honeycomb-dataset": "express-opentelemetry-app-2-dataset"

  # Sentry Exporters
  sentry/express-app:
    dsn: "https://a54f975c0645b4e41d3ff35a27a1a34f@o1161257.ingest.us.sentry.io/4505918888935424"
    environment: "prod"

  sentry/express-app-2:
    dsn: "https://d898d7274cfb1b9e94dd753f452b1722@o1161257.ingest.us.sentry.io/4508739229843456"
    environment: "prod"

processors:
  batch: {}

  memory_limiter:
    check_interval: 1s # Ensure this is greater than zero to avoid startup errors
    limit_percentage: 75 # Maximum memory usage before limiting
    spike_limit_percentage: 25 # Allow short-term spikes over limit

  routing:
    attribute_source: resource
    from_attribute: service.name
    default_exporters: [debug]
    table:
      - value: express-opentelemetry-app
        exporters: [otlphttp/honeycomb-express-app, sentry/express-app]
      - value: express-opentelemetry-app-2
        exporters: [otlphttp/honeycomb-express-app-2, sentry/express-app-2]

service:
  pipelines:
    traces:
      receivers: [otlp]
      processors: [memory_limiter, batch, routing]
      exporters:
        - debug
        - otlphttp/honeycomb-express-app
        - otlphttp/honeycomb-express-app-2
        - sentry/express-app
        - sentry/express-app-2
